{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "17X8nY5oZf31AwVkzbnnnmM640GC7-msB",
      "authorship_tag": "ABX9TyOFdx+nLtwGYJjNlmmjTTcN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haaaram/MS-AIschool/blob/main/DeepPiCar(Project)/object_detection_ref.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1: Mount Google drive"
      ],
      "metadata": {
        "id": "XDbERSeX227J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuBR-PAJCqQF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "model_dir = '/content/drive/MyDrive/Colab Notebooks/TransferLearning/Training'\n",
        "#!rm -rf '{model_dir}' ### model_dir 경로에 있는 파일 및 폴더 완전 제거\n",
        "#os.makedirs(model_dir, exist_ok=True)\n",
        "!ls -ltra '{model_dir}'/.. ### model_dir의 상위 디렉토리를 나열"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2: Configs and Hyperparameters"
      ],
      "metadata": {
        "id": "C-l7-xgk2wW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you forked the repository, you can replace the link.\n",
        "repo_url = 'https://github.com/dctian/DeepPiCar' ### 레포 주소. 필요한 데이터와 모델을 다운로드한다.\n",
        "\n",
        "# Number of training steps.(epoch 수)\n",
        "num_steps = 1000  # 200000\n",
        "#num_steps = 100  # 200000\n",
        "\n",
        "# Number of evaluation steps.\n",
        "num_eval_steps = 50\n",
        "\n",
        "\n",
        "# model configs are from Model Zoo github:\n",
        "# https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models\n",
        "MODELS_CONFIG = { ### 사전 훈련된 객체 감지 모델에 대한 설정을 포함하는 딕셔너리\n",
        "    #http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz\n",
        "    'ssd_mobilenet_v1_quantized': {\n",
        "        'model_name': 'ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18',\n",
        "        'pipeline_file': 'ssd_mobilenet_v1_quantized_300x300_coco14_sync.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    'ssd_mobilenet_v2': {\n",
        "        'model_name': 'ssd_mobilenet_v2_coco_2018_03_29',\n",
        "        'pipeline_file': 'ssd_mobilenet_v2_coco.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    #http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz\n",
        "    'ssd_mobilenet_v2_quantized': {\n",
        "        'model_name': 'ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03',\n",
        "        'pipeline_file': 'ssd_mobilenet_v2_quantized_300x300_coco.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    'faster_rcnn_inception_v2': {\n",
        "        'model_name': 'faster_rcnn_inception_v2_coco_2018_01_28',\n",
        "        'pipeline_file': 'faster_rcnn_inception_v2_pets.config',\n",
        "        'batch_size': 12\n",
        "    },\n",
        "    'rfcn_resnet101': {\n",
        "        'model_name': 'rfcn_resnet101_coco_2018_01_28',\n",
        "        'pipeline_file': 'rfcn_resnet101_pets.config',\n",
        "        'batch_size': 12\n",
        "    }\n",
        "}\n",
        "\n",
        "# Pick the model you want to use\n",
        "# Select a model in `MODELS_CONFIG`.\n",
        "# Note: for Edge TPU, you have to:\n",
        "# 1) start with a pretrained model from model zoo, such as above 4\n",
        "# 2) Must be a quantized model, which reduces the model size significantly\n",
        "selected_model = 'ssd_mobilenet_v2_quantized'\n",
        "\n",
        "# Name of the object detection model to use.\n",
        "MODEL = MODELS_CONFIG[selected_model]['model_name']\n",
        "\n",
        "# Name of the pipline file in tensorflow object detection API.\n",
        "pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n",
        "\n",
        "# Training batch size fits in Colabe's Tesla K80 GPU memory for selected model.\n",
        "batch_size = MODELS_CONFIG[selected_model]['batch_size']"
      ],
      "metadata": {
        "id": "xSElMovICtkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3: Set up Training Environment\n",
        "Clone the DeepPiCar repository or your fork."
      ],
      "metadata": {
        "id": "sdgEawn_2n4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "### 현재 위치는 /content\n",
        "%cd /content\n",
        "\n",
        "repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))\n",
        "'''\n",
        "os.path.abspath(path)\n",
        " 입력된 경로를 절대 경로(absolute path)로 변환한다.\n",
        "\n",
        "os.path.join(path1, path2, ...)\n",
        " 여러 개의 경로를 연결하여 하나의 경로를 생성한다.\n",
        "\n",
        "os.path.basename(path)\n",
        " 입력된 경로에서 파일 또는 디렉토리의 이름 부분을 추출한다.\n",
        "'''\n",
        "\n",
        "!git clone {repo_url}\n",
        "%cd {repo_dir_path}\n",
        "\n",
        "print('Pull it so that we have the latest code/data')\n",
        "# pull -> 원격 저장소의 소스를 가져오고 해당 소스가 현재 내 소스보다 더 최신이면\n",
        "# 지금의 버전을 해당 소스에 맞춰 올린다.\n",
        "!git pull"
      ],
      "metadata": {
        "id": "neBFy68JDBsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required packages"
      ],
      "metadata": {
        "id": "2RXs2sD_2jed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone --quiet https://github.com/tensorflow/models.git\n",
        "# --quiet -> 에러 및 경고 메시지를 제외한 모든 메시지 출력x\n",
        "\n",
        "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "'''\n",
        "apt-get\n",
        "  CLI 패키지 관리도구. 필요한 패키지를 설치한다.\n",
        "  pip는 파이썬 패키지만 설치할 수 있는 반면, apt-get은 모든 종류의 패키지를 설치한다.\n",
        "\n",
        "-qq\n",
        "  조용히 설치\n",
        "\n",
        "protpbuf-compiler\n",
        "  Protocol Buffers 컴파일러.\n",
        "  텐서플로우의 모델구조와 데이터를 직렬화 및 역직렬화하기 위해 사용된다.\n",
        "\n",
        "python-pil\n",
        "  Python Imaging Library(PIL) 또는 Pillow.\n",
        "  이미지 처리와 조작을 위한 라이브러리이다.\n",
        "\n",
        "python-lxml\n",
        "  lxml은 XML 및 HTML 파싱 및 처리를 위한 Python 라이브러리이다.\n",
        "  TensorFlow Object Detection API에서 주로 XML 파일을 다룰 때 사용한다.\n",
        "\n",
        "python-tk\n",
        "  Tkinter 라이브러리이다. GUI를 만들 때 사용된다.\n",
        "  TensorFlow Object Detection API에서 이미지 시각화 및 편집 시 사용된다.\n",
        "'''\n",
        "\n",
        "!pip install -q Cython contextlib2 pillow lxml matplotlib\n",
        "'''\n",
        "Cython\n",
        "  Python 프로그램을 C로 컴파일할 수 있는 도구.\n",
        "\n",
        "contextlib2\n",
        "  Python2에서 제공되는 contextlib 모듈을 Python3와 호환되도록 확장하는 라이브러리이다.\n",
        "'''\n",
        "\n",
        "!pip install -q pycocotools\n",
        "# pycocotools -> COCO 데이터셋을 처리하고 분석하기 위한 도구 모음\n",
        "\n",
        "%cd /content/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "'''\n",
        "!protoc\n",
        "  protocol buffer - 구글에서 개발한 직렬화 데이터 구조\n",
        "\n",
        "--python_out=.\n",
        "  현재 디렉토리('.')에 생성된 Python 파일을 저장한다.\n",
        "'''\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'\n",
        "\n",
        "!python object_detection/builders/model_builder_test.py\n",
        "# TensorFlow Object Detection API의 모델 빌더가 올바르게 구동되는지 확인"
      ],
      "metadata": {
        "id": "qu0t78bPDEbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare tfrecord files"
      ],
      "metadata": {
        "id": "YhkZ1mYq2cup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {repo_dir_path}/models/object_detection\n",
        "\n",
        "# Convert train folder annotation xml files to a single csv file,\n",
        "# generate the `label_map.pbtxt` file to `data/` directory as well.\n",
        "!python code/xml_to_csv.py -i data/images/train -o data/annotations/train_labels.csv -l data/annotations\n",
        "'''\n",
        "-i : 입력 디렉토리 지정\n",
        "-o : 출력 파일 경로 지정\n",
        "-l : 레이블 디렉토리 지정\n",
        "'''\n",
        "\n",
        "# Convert test folder annotation xml files to a single csv.\n",
        "!python code/xml_to_csv.py -i data/images/test -o data/annotations/test_labels.csv\n",
        "\n",
        "# Generate `train.record`\n",
        "!python code/generate_tfrecord.py --csv_input=data/annotations/train_labels.csv --output_path=data/annotations/train.record --img_path=data/images/train --label_map data/annotations/label_map.pbtxt\n",
        "\n",
        "# Generate `test.record`\n",
        "!python code/generate_tfrecord.py --csv_input=data/annotations/test_labels.csv --output_path=data/annotations/test.record --img_path=data/images/test --label_map data/annotations/label_map.pbtxt"
      ],
      "metadata": {
        "id": "FKngSEovDKsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_record_fname = repo_dir_path + '/models/object_detection/data/annotations/test.record'\n",
        "train_record_fname = repo_dir_path + '/models/object_detection/data/annotations/train.record'\n",
        "label_map_pbtxt_fname = repo_dir_path + '/models/object_detection/data/annotations/label_map.pbtxt'"
      ],
      "metadata": {
        "id": "BaFXja4BDiPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat data/annotations/test_labels.csv\n",
        "# 파일의 내용 출력\n",
        "# cat -> concatenate, 파일을 연결하거나 파일의 내용을 출력할 때 사용"
      ],
      "metadata": {
        "id": "srGu9TYvDkU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download base model"
      ],
      "metadata": {
        "id": "9cca_LBy2V1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/models/research\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import urllib.request\n",
        "import tarfile\n",
        "\n",
        "MODEL_FILE = MODEL + '.tar.gz' # 'Tape Archive'로 묶인 후 'gzip' 알고리즘으로 압축됨\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "DEST_DIR = '/content/models/research/pretrained_model'\n",
        "\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "    # urllib.request.urlretrieve -> 인터넷에서 파일을 다운로드한다.\n",
        "\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "os.remove(MODEL_FILE)\n",
        "if (os.path.exists(DEST_DIR)):\n",
        "    shutil.rmtree(DEST_DIR)\n",
        "    # 해당 디렉토리와 하위 디렉토리 및 파일을 모두 삭제\n",
        "os.rename(MODEL, DEST_DIR)"
      ],
      "metadata": {
        "id": "ENdljQCzDm-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo {DEST_DIR}\n",
        "!ls -alh {DEST_DIR}\n",
        "'''\n",
        "-a : 숨겨진 파일과 디렉토리도 포함하여 모든 파일과 디렉토리 표시\n",
        "-ㅣ: 파일 및 디렉토리의 세부 정보(권한, 소유자, 크기 등)를 표시한다.\n",
        "-h : 파일 크기를 인간이 읽기 쉬운 형식으로 표시한다.\n",
        "'''"
      ],
      "metadata": {
        "id": "MhXdNpqlDp1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune_checkpoint = os.path.join(DEST_DIR, \"model.ckpt\")\n",
        "fine_tune_checkpoint"
      ],
      "metadata": {
        "id": "jHKb5pb5Dr9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Transfer Learning Training\n",
        "Configuring a Training Pipeline"
      ],
      "metadata": {
        "id": "KPIexdgM2EWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "pipeline_fname = os.path.join('/content/models/research/object_detection/samples/configs/', pipeline_file)\n",
        "\n",
        "assert os.path.isfile(pipeline_fname), '`{}` not exist'.format(pipeline_fname)\n",
        "# assert: 파일이 존재하는지 확인\n",
        "# 'assert 조건, 메시지': 주어진 조건을 확인하고, 조건이 False일 때 메시지가 출력된다."
      ],
      "metadata": {
        "id": "orltSlOxDvrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_num_classes(pbtxt_fname):\n",
        "    from object_detection.utils import label_map_util\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    return len(category_index.keys())"
      ],
      "metadata": {
        "id": "GYEmWC4CDyHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# training pipeline file defines:\n",
        "# - pretrain model path\n",
        "# - the train/test sets\n",
        "# - ID to Label mapping and number of classes\n",
        "# - training batch size\n",
        "# - epochs to trains\n",
        "# - learning rate\n",
        "# - etc\n",
        "\n",
        "# note we just need to use a sample one, and make edits to it.\n",
        "\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
        "with open(pipeline_fname) as f:\n",
        "    s = f.read()\n",
        "with open(pipeline_fname, 'w') as f:\n",
        "\n",
        "    # fine_tune_checkpoint: downloaded pre-trained model checkpoint path\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
        "\n",
        "    # tfrecord files train and test, we created earlier with our training/test sets\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(train.record)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(val.record)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s)\n",
        "\n",
        "    # label_map_path: ID to label file\n",
        "    s = re.sub(\n",
        "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
        "\n",
        "    # Set training batch_size.\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "\n",
        "    # Set training steps, num_steps (Number of epochs to train)\n",
        "    s = re.sub('num_steps: [0-9]+',\n",
        "               'num_steps: {}'.format(num_steps), s)\n",
        "\n",
        "    # Set number of classes num_classes.\n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_classes), s)\n",
        "    f.write(s)\n",
        "\n",
        "'''\n",
        "re.sub(pattern, replacement, string)\n",
        "  pattern: 대체할 부분을 찾기 위한 정규 표현식 패턴\n",
        "  replacement: 대체할 문자열\n",
        "  string: 대체 작업을 수행할 대상 문자열\n",
        "'''"
      ],
      "metadata": {
        "id": "kWtB3KF0D3AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat {label_map_pbtxt_fname}"
      ],
      "metadata": {
        "id": "RjwevqHjD9S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look for num_classes: 6, since we have 5 different road signs and 1 person type (total of 6 types)\n",
        "!cat {pipeline_fname}"
      ],
      "metadata": {
        "id": "hqCrXOhyD_rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Tensorboard(Optional)"
      ],
      "metadata": {
        "id": "-8fJNBtU3Bl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "# wget -> 웹에서 파일을 다운로드한다.\n",
        "!unzip -o ngrok-stable-linux-amd64.zip\n",
        "# -o -> 덮어쓰기(overwrite).\n",
        "#       zip 파일을 해제할 때 기존 파일을 덮어쓸지 묻지 않고 자동으로 덮어쓴다."
      ],
      "metadata": {
        "id": "F25GB3WZEC67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_DIR = model_dir\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir \"{}\" --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "'''\n",
        "get_ipython().system_raw()\n",
        "  Jupyter Notebook 또는 IPython 환경에서 명령줄 명령을 실행하는 데 사용\n",
        "\n",
        "'tensorboard ...'\n",
        "  TensorBoard를 실행한다.\n",
        "\n",
        "--logdir \"{}\"\n",
        "  로그 및 모델 파일의 디렉토리를 지정한다. LOG_DIR 내용이 {}부분에 들어간다\n",
        "\n",
        "--host 0.0.0.0\n",
        "  TensorBoard를 0.0.0.0 IP 주소로 실행하여 외부에서 접근할 수 있도록 한다.\n",
        "\n",
        "--port 6006\n",
        "  TensorBoard 웹 인터페이스를 위한 포트 번호를 지정한다.\n",
        "\n",
        "& : 백그라운드에서 TensorBoard 실행한다.\n",
        "'''"
      ],
      "metadata": {
        "id": "AboqFYR0EEvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "# ngrok을 사용하여 로컬 포트 6006을 외부로 공개한다."
      ],
      "metadata": {
        "id": "KvfJsacqEH4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get Tensorboard link"
      ],
      "metadata": {
        "id": "34zJ1zq23H07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
        "'''\n",
        "curl\n",
        "  로컬 ngrok 서버에서 터널 정보를 가져온다. -s는 silent\n",
        "\n",
        "|: 명령어를 연결함. 앞의 명령어의 출력을 뒤의 명령어의 입력으로 전달한다?\n",
        "\n",
        "-c:  코드를 실행하도록 지시하는 옵션\n",
        "'''"
      ],
      "metadata": {
        "id": "-BqPdqmDEMeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "kBPtT0vp3Ka7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################### SEND ALERT EMAIL AT FINISH WITH GMAIL #####################\n",
        "# To send email from Python from your google account, MUST\n",
        "# 1) Enable less secure app\n",
        "# https://myaccount.google.com/lesssecureapps\n",
        "# 2) Disable Unlock Capcha\n",
        "# https://accounts.google.com/b/0/DisplayUnlockCaptcha\n",
        "\n",
        "import smtplib\n",
        "\n",
        "def SendEmail(msg):\n",
        "    with open('/content/gdrive/My Drive/Colab Notebooks/pw.txt') as file:\n",
        "        data = file.readlines()\n",
        "\n",
        "    gmail_user = 'david.tian@gmail.com'\n",
        "    gmail_password = data[0]\n",
        "\n",
        "\n",
        "    sent_from = gmail_user\n",
        "    to = ['dctian@hotmail.com']\n",
        "    subject = msg\n",
        "    body = '%s\\n\\n- David' % msg\n",
        "\n",
        "    email_text = \\\n",
        "\"\"\"From: %s\n",
        "To: %s\n",
        "Subject: %s\n",
        "\n",
        "%s\n",
        "\"\"\" % (sent_from, \", \".join(to), subject, body)\n",
        "\n",
        "    server = smtplib.SMTP(\"smtp.gmail.com\", 587)\n",
        "    server.ehlo()\n",
        "    server.starttls()\n",
        "    server.login(gmail_user, gmail_password)\n",
        "    server.sendmail(sent_from, to, email_text)\n",
        "    server.quit()\n",
        "\n",
        "    print(f'Email: \\n{email_text}')"
      ],
      "metadata": {
        "id": "YA4DSFJDESEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_steps = 2000\n",
        "SendEmail(\"Colab train started\")\n",
        "!python /content/models/research/object_detection/model_main.py \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --model_dir='{model_dir}' \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps} \\\n",
        "    --num_eval_steps={num_eval_steps}\n",
        "# --alsologtostderr\n",
        "#   로그 메시지가 터미널에 표시되고,\n",
        "#   표준 출력('stdout')과 표준 오류('stderr')로 분리되지 않고 표시된다.\n",
        "SendEmail(\"Colab train finished\")"
      ],
      "metadata": {
        "id": "dUeWLhDLEVbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltra '{model_dir}'\n",
        "'''\n",
        "-ltra 옵션 -> l(long format), t(newest first), r(reverse order), a(all files)\n",
        "'''"
      ],
      "metadata": {
        "id": "BNfgo3dxEY5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 5: Save and Convert Model Output\n",
        "Exporting a Trained Inference Graph"
      ],
      "metadata": {
        "id": "H_5Tshn63Ra5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "output_directory = '%s/fine_tuned_model' % model_dir\n",
        "os.makedirs(output_directory, exist_ok=True)"
      ],
      "metadata": {
        "id": "XpntGk7SEc_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lst = os.listdir(model_dir)\n",
        "# model_dir 안에 있는 파일과 서브 디렉토리의 목록을 가져옴\n",
        "\n",
        "# find the last model checkpoint file, i.e. model.ckpt-1000.meta\n",
        "lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n",
        "# 'l'문자열에 'model.ckpt-'과 '.meta'가 포함되어 있는지\n",
        "steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n",
        "\n",
        "last_model = lst[steps.argmax()].replace('.meta', '')\n",
        "\n",
        "last_model_path = os.path.join(model_dir, last_model)\n",
        "print(last_model_path)"
      ],
      "metadata": {
        "id": "0u_NofU1Efom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo creates the frozen inference graph in fine_tune_model\n",
        "# there is an \"Incomplete shape\" message.  but we can safely ignore that.\n",
        "!python /content/models/research/object_detection/export_inference_graph.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --output_directory='{output_directory}' \\\n",
        "    --trained_checkpoint_prefix='{last_model_path}'"
      ],
      "metadata": {
        "id": "C-42NJUhEh-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193\n",
        "# create the tensorflow lite graph\n",
        "!python /content/models/research/object_detection/export_tflite_ssd_graph.py \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --trained_checkpoint_prefix='{last_model_path}' \\\n",
        "    --output_directory='{output_directory}' \\\n",
        "    --add_postprocessing_op=true"
      ],
      "metadata": {
        "id": "qSiofhZKEmBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"CONVERTING frozen graph to quantized TF Lite file...\"\n",
        "!tflite_convert \\\n",
        "  --output_file='{output_directory}/road_signs_quantized.tflite' \\\n",
        "  --graph_def_file='{output_directory}/tflite_graph.pb' \\\n",
        "  --inference_type=QUANTIZED_UINT8 \\\n",
        "  --input_arrays='normalized_input_image_tensor' \\\n",
        "  --output_arrays='TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3' \\\n",
        "  --mean_values=128 \\\n",
        "  --std_dev_values=128 \\\n",
        "  --input_shapes=1,300,300,3 \\\n",
        "  --change_concat_input_ranges=false \\\n",
        "  --allow_nudging_weights_to_use_fast_gemm_kernel=true \\\n",
        "  --allow_custom_ops"
      ],
      "metadata": {
        "id": "SziH21iOEoSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"CONVERTING frozen graph to unquantized TF Lite file...\"\n",
        "!tflite_convert \\\n",
        "  --output_file='{output_directory}/road_signs_float.tflite' \\\n",
        "  --graph_def_file='{output_directory}/tflite_graph.pb' \\\n",
        "  --input_arrays='normalized_input_image_tensor' \\\n",
        "  --output_arrays='TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3' \\\n",
        "  --mean_values=128 \\\n",
        "  --std_dev_values=128 \\\n",
        "  --input_shapes=1,300,300,3 \\\n",
        "  --change_concat_input_ranges=false \\\n",
        "  --allow_nudging_weights_to_use_fast_gemm_kernel=true \\\n",
        "  --allow_custom_ops"
      ],
      "metadata": {
        "id": "Gk6966wPEqls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_directory)\n",
        "!ls -ltra '{output_directory}'\n",
        "#pb_fname = os.path.join(os.path.abspath(output_directory), \"frozen_inference_graph.pb\") # this is main one\n",
        "pb_fname = os.path.join(os.path.abspath(output_directory), \"frozen_inference_graph.pb\")  # this is tflite graph\n",
        "!cp '{label_map_pbtxt_fname}' '{output_directory}'"
      ],
      "metadata": {
        "id": "Y-0hdynTEx9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run inference test"
      ],
      "metadata": {
        "id": "pmywyBnL3mpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
        "PATH_TO_CKPT = pb_fname\n",
        "print(PATH_TO_CKPT)\n",
        "\n",
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = label_map_pbtxt_fname\n",
        "\n",
        "# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n",
        "PATH_TO_TEST_IMAGES_DIR =  os.path.join(repo_dir_path, \"models/object_detection/data/images/test\")\n",
        "\n",
        "assert os.path.isfile(pb_fname)\n",
        "assert os.path.isfile(PATH_TO_LABELS)\n",
        "TEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, \"*.jpg\"))\n",
        "assert len(TEST_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_TEST_IMAGES_DIR)\n",
        "print(TEST_IMAGE_PATHS)"
      ],
      "metadata": {
        "id": "aJxttHRwE04K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/models/research/object_detection\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "\n",
        "# This is needed to display the images.\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(\n",
        "    label_map, max_num_classes=num_classes, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "\n",
        "def load_image_into_numpy_array(image):\n",
        "    (im_width, im_height) = image.size\n",
        "    return np.array(image.getdata()).reshape(\n",
        "        (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "# Size, in inches, of the output images.\n",
        "IMAGE_SIZE = (12, 8)\n",
        "\n",
        "\n",
        "def run_inference_for_single_image(image, graph):\n",
        "    with graph.as_default():\n",
        "        with tf.Session() as sess:\n",
        "            # Get handles to input and output tensors\n",
        "            ops = tf.get_default_graph().get_operations()\n",
        "            all_tensor_names = {\n",
        "                output.name for op in ops for output in op.outputs}\n",
        "            tensor_dict = {}\n",
        "            for key in [\n",
        "                'num_detections', 'detection_boxes', 'detection_scores',\n",
        "                'detection_classes', 'detection_masks'\n",
        "            ]:\n",
        "                tensor_name = key + ':0'\n",
        "                if tensor_name in all_tensor_names:\n",
        "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "                        tensor_name)\n",
        "            if 'detection_masks' in tensor_dict:\n",
        "                # The following processing is only for single image\n",
        "                detection_boxes = tf.squeeze(\n",
        "                    tensor_dict['detection_boxes'], [0])\n",
        "                detection_masks = tf.squeeze(\n",
        "                    tensor_dict['detection_masks'], [0])\n",
        "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "                real_num_detection = tf.cast(\n",
        "                    tensor_dict['num_detections'][0], tf.int32)\n",
        "                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n",
        "                                           real_num_detection, -1])\n",
        "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n",
        "                                           real_num_detection, -1, -1])\n",
        "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
        "                detection_masks_reframed = tf.cast(\n",
        "                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "                # Follow the convention by adding back the batch dimension\n",
        "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "                    detection_masks_reframed, 0)\n",
        "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "            # Run inference\n",
        "            output_dict = sess.run(tensor_dict,\n",
        "                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "            output_dict['num_detections'] = int(\n",
        "                output_dict['num_detections'][0])\n",
        "            output_dict['detection_classes'] = output_dict[\n",
        "                'detection_classes'][0].astype(np.uint8)\n",
        "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "            if 'detection_masks' in output_dict:\n",
        "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "    return output_dict"
      ],
      "metadata": {
        "id": "xAWrAW0LE4Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# running inferences.  This should show images with bounding boxes\n",
        "%matplotlib inline\n",
        "\n",
        "print('Running inferences on %s' % TEST_IMAGE_PATHS)\n",
        "for image_path in TEST_IMAGE_PATHS:\n",
        "    image = Image.open(image_path)\n",
        "    # the array based representation of the image will be used later in order to prepare the\n",
        "    # result image with boxes and labels on it.\n",
        "    image_np = load_image_into_numpy_array(image)\n",
        "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "    # Actual detection.\n",
        "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "    # Visualization of the results of a detection.\n",
        "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np,\n",
        "        output_dict['detection_boxes'],\n",
        "        output_dict['detection_classes'],\n",
        "        output_dict['detection_scores'],\n",
        "        category_index,\n",
        "        instance_masks=output_dict.get('detection_masks'),\n",
        "        use_normalized_coordinates=True,\n",
        "        line_thickness=2)\n",
        "    plt.figure(figsize=IMAGE_SIZE)\n",
        "    plt.imshow(image_np)"
      ],
      "metadata": {
        "id": "17e11BtnFLn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert to Edge TPU's tflite Format"
      ],
      "metadata": {
        "id": "-ZjTcHub3uwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download this file from google drive.\n",
        "!ls -lt '/content/gdrive/My Drive/Colab Notebooks/TransferLearning/Training/fine_tuned_model/road_signs_quantized.tflite'"
      ],
      "metadata": {
        "id": "LFKkgQGjFO1N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
